{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the modules os and pandas and the classes ML_GRB and RND_FOREST from the ML_GRB.py file.\n",
    "I will use \"os\" to take the path of the working folder for the data reading part, while \"pandas\" to manage the DataFrame.\n",
    "\n",
    "The class ML_GRB is the main class of the project, while RND_FOREST is the class for the specific model to use in the main class \"ML_GRB\". The idea is to create other model classes like \"RND_FOREST\" using similiar structure betwen the model class and the main class (see \"ML_GRB.py\" for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ML_GRB import ML_GRB, RND_FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I intitialize the main class and save it in the variable \"obj\". Then I use the function \n",
    "DataReading (see functions.py) to read the data from the folder \"data\". This function takes in \n",
    "input the working folder path and saves the data as istances in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_path = os.path.dirname(os.path.abspath(__file__))   # directory path (This doesn't \n",
    "#                                                           work in python notebook, but it\n",
    "#                                                           works very well in python files, \n",
    "#                                                           so here I use the line below, and\n",
    "#                                                           one can change it if necessary.)\n",
    "dir_path = ''       # directory path\n",
    "\n",
    "obj = ML_GRB()      # initializing the main class ---> see ML_GRB.py\n",
    "obj.DataReading(dir_path)       # reading the data ---> see functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I build the dataset to work with.\n",
    "First I take the DataFrames of spectral and energetic parameters and I save they in two variables.\n",
    "Then I take just the features useful for the model (for now I take the features below for a test but the code should works fine even with all the necessary variables of the dataset).\n",
    "After that I concatenate the two DataFrame to a single DataFrame tha will works as our \n",
    "dataset for the model.\n",
    "Here I choose to take only the data fitted with the CPL model just for a test, but honestly \n",
    "the implication of this are a bit confusing for me. This could be one of the theme for a possible \n",
    "next meeting.\n",
    "\n",
    "Finally one can filter the dataset again changing the features list.\n",
    "I've done this because in my mind the first filter is useful to select the features usable in general, \n",
    "while the last filter is to select the specific features to use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the dataframes from the class\n",
    "spec_par = obj.spectral_data\n",
    "energ_par = obj.energetic_data\n",
    "\n",
    "# Defining the features to use in the model from the two dataframes\n",
    "filter_ener = ['z', 'S', 'Tp64', 'Fp64', 'Eiso','Liso']\n",
    "filter_spec = ['tstart', 'DeltaT', 'alpha', 'Ep', 'F']\n",
    "\n",
    "spec_par_reset = spec_par.reset_index(drop=True)  # Reset index and drop the old index column\n",
    "\n",
    "# Concatenating the two dataframe to form one and filtering\n",
    "df1 = energ_par[energ_par['ID'] != 'GRB080413B'][filter_ener]   # Exluding GRB080413B becuase 'SMOD' = 'PL'\n",
    "df2 = spec_par_reset[(spec_par['SMod'] == 'CPL') & (spec_par['SType'] == 'i')][filter_spec]    # Using the CPL SMod\n",
    "df_temp = pd.concat([df1, df2], axis=0)     # Concatenating\n",
    "df = df_temp.apply(lambda x: pd.Series(x.dropna().values))      # Dropping the Nan values\n",
    "\n",
    "# Selecting the features to use (one can change directly from here if he wants)\n",
    "features = ['z', 'S', 'Tp64', 'Fp64', 'Eiso','Liso', \n",
    "          'tstart', 'DeltaT', 'alpha', 'Ep', 'F']\n",
    "dataset = df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I initialize the model class and save it in the variable \"model\", then \n",
    "I set the model in the main class (see ML_GRB.py). \n",
    "After that I train the model with the function Run (see functions.py).\n",
    "\n",
    "I had set a random_state of 42 to split training and testing data in the same way every time \n",
    "I run the code, just to hate always the same results in this first \"code testing\" moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizialising the model class ---> see ML_GRB.py\n",
    "model = RND_FOREST()\n",
    "obj.SetModel(model)     # Setting the model in the main class\n",
    "\n",
    "# Fitting the model ---> see functions.py\n",
    "obj.model.Run(dataset, train_size = 0.66, random_state = 42,\n",
    "              n_estimators = 500, max_depth = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last cell I print the results: \n",
    "- the values of redshift from the test data compared with the values of redshift predicted by the model\n",
    "- the score on the test data\n",
    "- the score on the train data\n",
    "\n",
    "In the next future I want to implement some consinstent tools to evaluate the model.\n",
    "For now I have printed only simple results that doesn't has much estimation values, just\n",
    "to test the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z test: 2.770\t\tz predicted: 2.888\n",
      "z test: 0.360\t\tz predicted: 0.362\n",
      "z test: 0.360\t\tz predicted: 0.364\n",
      "z test: 0.540\t\tz predicted: 0.536\n",
      "z test: 3.600\t\tz predicted: 3.750\n",
      "z test: 2.770\t\tz predicted: 2.825\n",
      "z test: 2.690\t\tz predicted: 2.695\n",
      "z test: 0.760\t\tz predicted: 0.717\n",
      "z test: 4.350\t\tz predicted: 3.937\n",
      "z test: 3.000\t\tz predicted: 3.131\n",
      "z test: 1.410\t\tz predicted: 1.422\n",
      "z test: 2.030\t\tz predicted: 2.075\n",
      "z test: 2.300\t\tz predicted: 2.298\n",
      "z test: 1.020\t\tz predicted: 1.021\n",
      "z test: 0.690\t\tz predicted: 0.703\n",
      "z test: 1.330\t\tz predicted: 1.297\n",
      "z test: 0.590\t\tz predicted: 0.529\n",
      "z test: 0.690\t\tz predicted: 0.703\n",
      "z test: 0.820\t\tz predicted: 0.822\n",
      "z test: 1.320\t\tz predicted: 1.304\n",
      "z test: 2.610\t\tz predicted: 2.530\n",
      "z test: 2.400\t\tz predicted: 2.461\n",
      "z test: 2.900\t\tz predicted: 2.946\n",
      "z test: 0.890\t\tz predicted: 0.887\n",
      "z test: 1.030\t\tz predicted: 1.036\n",
      "z test: 1.720\t\tz predicted: 1.765\n",
      "z test: 2.490\t\tz predicted: 2.469\n",
      "z test: 1.610\t\tz predicted: 1.596\n",
      "z test: 0.110\t\tz predicted: 0.186\n",
      "z test: 2.200\t\tz predicted: 2.169\n",
      "z test: 1.000\t\tz predicted: 0.964\n",
      "z test: 2.040\t\tz predicted: 2.074\n",
      "z test: 1.640\t\tz predicted: 1.608\n",
      "z test: 0.850\t\tz predicted: 0.851\n",
      "z test: 1.260\t\tz predicted: 1.272\n",
      "z test: 5.000\t\tz predicted: 4.025\n",
      "z test: 0.920\t\tz predicted: 0.926\n",
      "z test: 0.610\t\tz predicted: 0.619\n",
      "z test: 1.310\t\tz predicted: 1.290\n",
      "z test: 0.810\t\tz predicted: 0.825\n",
      "z test: 1.240\t\tz predicted: 1.257\n",
      "z test: 0.900\t\tz predicted: 0.899\n",
      "z test: 1.710\t\tz predicted: 1.742\n",
      "z test: 1.480\t\tz predicted: 1.483\n",
      "z test: 4.500\t\tz predicted: 3.954\n",
      "z test: 1.800\t\tz predicted: 1.779\n",
      "z test: 3.290\t\tz predicted: 3.608\n",
      "z test: 0.810\t\tz predicted: 0.817\n",
      "z test: 2.100\t\tz predicted: 2.093\n",
      "z test: 2.200\t\tz predicted: 2.175\n",
      "z test: 0.800\t\tz predicted: 0.820\n",
      "The test score is : 0.974 while the train score is: 0.998\n"
     ]
    }
   ],
   "source": [
    "# Printing the results\n",
    "test_score = obj.model.forest.score(obj.model.X_test, obj.model.y_test)\n",
    "train_score = obj.model.forest.score(obj.model.X_train, obj.model.y_train)\n",
    "\n",
    "for a, b in zip(obj.model.y_test, obj.model.y_pred):\n",
    "    print('z test: {:1.3f}\\t\\tz predicted: {:1.3f}'.format(a, b))\n",
    "    \n",
    "print('The test score is : {:.3f} while the train score is: {:.3f}'.format(test_score, train_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
